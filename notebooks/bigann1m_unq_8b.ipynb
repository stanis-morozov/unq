{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "%env OMP_NUM_THREADS=56\n",
    "#^--change this\n",
    "import os, sys, time\n",
    "sys.path.insert(0, '..')\n",
    "import faiss\n",
    "import lib\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from qhoptim.pyt import QHAdam\n",
    "from functools import partial\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "\n",
    "experiment_name = 'bigann1m_unq_8b'\n",
    "experiment_name = '{}_{}.{:0>2d}.{:0>2d}_{:0>2d}:{:0>2d}:{:0>2d}'.format(experiment_name, *time.gmtime()[:6])\n",
    "print(\"experiment:\", experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This experiment trains UNQ model with 8-byte encoding on BIGANN1M dataset and standard model size.\n",
    "\n",
    "The original experiment was trained on 4 Nnvdia 1080 gtx GPUs. One can set arbitrary number of GPUs by changing the `CUDA_VISIBLE_DEVICES` variable at the top. This experiment may also consume around 8Gb of GPU memory. If one doesn't have this much memory, there are ways to reduce memory usage at the cost of training or inference speed.\n",
    "\n",
    "1. Split data into smaller chunks. One can reduce `batch_size` parameter in `LearnedSimilaritySearch` and call lib.free_memory() after every memory-intensive operation. Use virtual batching: change `trainer.train_on_batch` below to:\n",
    "```(python3.5)\n",
    "model.train(True)\n",
    "trainer.opt.zero_grad()\n",
    "for chunk_start in range(0, len(x_batch), 1000):\n",
    "    chunk = slice(chunk_start, chunk_start + 1000)\n",
    "    metrics_t = model.compute_loss(x_batch[chunk], \n",
    "        sample_uniform(train_base, nearest_ids[chunk]),\n",
    "        sample_uniform(train_base, negative_ids[chunk])\n",
    "    )\n",
    "    metrics_t['loss'].mean().backward()\n",
    "trainer.opt.step()\n",
    "trainer.step += 1\n",
    "...\n",
    "```\n",
    "2. Reduce __batch_size__ in the __iterate_minibatches__ function - this will significantly reduce memory requirements but may reduce the resulting recall. Please also consider reducing `learning_rate_base` and `increasing warmup_steps` in the optimizer.\n",
    "3. If you're low on CPU (we had 56), neighbor lookup will take up a lot of time. Set `rerank_k=float('inf')` - this will cause nearest neighbors to be computed with less memory-hungry implementation of FAISSFlatIndex. Please note that this will cause __overestimation__ of recall. Do not use this for comparison with other models.  \n",
    "\n",
    "By default, this notebook trains a model that reranks 500 most promising candidates during nearest neighbor search. You can increase this number to boost recall across all regions. In order to do so, go to Trainer initialization below and change `rerank_k` in LearnedSimilaritySearch from 500 to a greater value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lib.Dataset('BIGANN1M', normalize=True)\n",
    "\n",
    "model = lib.UNQModel(\n",
    "    input_dim=dataset.vector_dim, hidden_dim=1024, bottleneck_dim=256,\n",
    "    encoder_layers=2, decoder_layers=2, Activation=nn.ReLU,\n",
    "    num_codebooks=8, codebook_size=256, initial_entropy=3.0,\n",
    "    share_codewords=True\n",
    ").cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(dataset.train_vectors[:1000].cuda())\n",
    "    # ^-- initialize model on first run\n",
    "\n",
    "trainer = lib.Trainer(\n",
    "    model=model, experiment_name=experiment_name, verbose=True,\n",
    "    Loss=lib.TripletLoss, loss_opts=dict(\n",
    "        reconstruction_distance=lib.DISTANCES['euclidian_squared'],\n",
    "        reconstruction_coeff=1.0, triplet_coeff=0.01, triplet_delta=0.1,\n",
    "        cv_coeff=0.1, square_cv=True,\n",
    "    ),\n",
    "    optimizer=lib.OneCycleSchedule(\n",
    "        QHAdam(model.parameters(), nus=(0.8, 0.7), betas=(0.95, 0.998)), \n",
    "        learning_rate_base=1e-3, warmup_steps=10000, decay_rate=0.2),\n",
    "    max_norm=10,\n",
    "    \n",
    "    LearnedSimilaritySearch=partial(lib.UNQSearch, model=model, rerank_k=500, batch_size=1000,\n",
    "                                    reorder_batch_size=250, device_ids=device_ids),\n",
    "    NegativeSimilaritySearch=partial(lib.UNQSearch, model=model, rerank_k=1, batch_size=1000,\n",
    "                                    reorder_batch_size=250, device_ids=device_ids),\n",
    "    SimilaritySearch=lib.FAISSFlatIndex,  # reference nearest vectors will be mined this way\n",
    "    device_ids=device_ids\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "best_recall = 0.0\n",
    "step_history, loss_history, recall_history = [], [], []\n",
    "\n",
    "train_base = dataset.train_vectors.cuda()\n",
    "train_gt = trainer.get_true_nearest_ids(train_base, k=10, exclude_self=True)\n",
    "\n",
    "def fetch_negatives():\n",
    "    return trainer.get_negative_ids(train_base.cpu(), positive_ids=train_gt.cpu(), k=100, skip_k=100).cuda()\n",
    "\n",
    "def sample_uniform(base, ids):\n",
    "    return base[ids[torch.arange(ids.shape[0]), torch.randint(0, ids.shape[1], size=[ids.shape[0]])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch_i in range(10000):\n",
    "    trainer.loss.module.opts['cv_coeff'] = max(0.05, 1. - trainer.step / 100_000)\n",
    "    trainer.drop_large_grads = trainer.step >= 1000\n",
    "    \n",
    "    lib.free_memory()\n",
    "    if epoch_i % 8 == 0: # every ~250 steps\n",
    "        train_negatives = fetch_negatives()\n",
    "    \n",
    "    for x_batch, nearest_ids, negative_ids in lib.iterate_minibatches(\n",
    "        train_base, train_gt, train_negatives, batch_size=8192 * 2, callback=tqdm):\n",
    "        metrics_t = trainer.train_on_batch(x_batch,\n",
    "                                           x_positives=sample_uniform(train_base, nearest_ids),\n",
    "                                           x_negatives=sample_uniform(train_base, negative_ids))\n",
    "        loss_history.append(metrics_t['loss'].mean().item())\n",
    "        step_history.append(trainer.step)\n",
    "    \n",
    "    if epoch_i % 10 == 0: # every ~500 steps\n",
    "        metrics_t = {key: lib.check_numpy(value) for key, value in metrics_t.items()}\n",
    "        lib.free_memory()\n",
    "        \n",
    "        recall_t = trainer.evaluate_recall(dataset.test_vectors.cuda(), dataset.query_vectors.cuda(), k=1)\n",
    "        # ^-- this line evaluates recall@1. Change k to 10, 100, etc. to get recall for different top sizes\n",
    "        \n",
    "        recall_history.append(recall_t)\n",
    "        if recall_t > best_recall:\n",
    "            best_recall = recall_t\n",
    "            trainer.save_checkpoint('best')\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[18, 6])\n",
    "        plt.subplot(1, 3, 1); plt.title('train loss'); plt.grid()\n",
    "        plt.scatter(step_history, loss_history, alpha=0.1)\n",
    "        plt.plot(moving_average(loss_history, span=100), c='orange')\n",
    "        plt.subplot(1, 3, 2); plt.title('dev recall@1'); plt.grid()\n",
    "        plt.plot(recall_history)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"step = %i \\t mean loss = %.5f \\t lr = %.5f \\t best dev recall = %.5f\" % (\n",
    "            trainer.step, np.mean(loss_history[-100:]),\n",
    "            lib.get_learning_rate(trainer.opt), best_recall))\n",
    "        for k, v in metrics_t.items():\n",
    "            print('{} = {}'.format(k, np.mean(lib.check_numpy(v))))\n",
    "\n",
    "# see you in tensorboard..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
